"""
BizBuySell Florida Scraper
ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ¾Ğ² Ğ² Florida
"""

import asyncio
import json
import csv
from datetime import datetime
from playwright.async_api import async_playwright
import random

class BizBuySellFloridaScraper:
    def __init__(self):
        self.base_url = "https://www.bizbuysell.com"
        self.businesses = []
        
        # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¾ĞºÑ€ÑƒĞ³Ğ¾Ğ² Florida Ğ¿Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ (Ñ‚Ğ¾Ğ¿-20 Ğ¿Ğ¾ Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ñ)
        self.counties = {
            "south_florida": [
                {"id": "miami-dade", "name": "Miami-Dade County", "population": 2716940},
                {"id": "broward", "name": "Broward County", "population": 1944375},
                {"id": "palm-beach", "name": "Palm Beach County", "population": 1496770},
                {"id": "collier", "name": "Collier County", "population": 384902},
                {"id": "lee", "name": "Lee County", "population": 760822},
                {"id": "monroe", "name": "Monroe County", "population": 82874}
            ],
            "central_florida": [
                {"id": "hillsborough", "name": "Hillsborough County", "population": 1459762},
                {"id": "orange", "name": "Orange County", "population": 1429908},
                {"id": "pinellas", "name": "Pinellas County", "population": 959107},
                {"id": "polk", "name": "Polk County", "population": 725046},
                {"id": "brevard", "name": "Brevard County", "population": 606612},
                {"id": "volusia", "name": "Volusia County", "population": 553543},
                {"id": "seminole", "name": "Seminole County", "population": 471826},
                {"id": "osceola", "name": "Osceola County", "population": 388656},
                {"id": "pasco", "name": "Pasco County", "population": 561891},
                {"id": "manatee", "name": "Manatee County", "population": 403253},
                {"id": "sarasota", "name": "Sarasota County", "population": 434006}
            ],
            "north_florida": [
                {"id": "duval", "name": "Duval County", "population": 995567},
                {"id": "leon", "name": "Leon County", "population": 293582},
                {"id": "st-johns", "name": "St. Johns County", "population": 273425}
            ]
        }
        
        # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ· config.json Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ defaults)
        self.config = {
            "selected_counties": [],
            "excluded_categories": [],
            "max_pages_per_county": 5
        }
        
    def load_config(self, config_file='scraper_config.json'):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                loaded_config = json.load(f)
                self.config.update(loaded_config)
                print(f"âœ… ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ¸Ğ· {config_file}")
                print(f"   Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ¾ Ğ¾ĞºÑ€ÑƒĞ³Ğ¾Ğ²: {len(self.config['selected_counties'])}")
                print(f"   Ğ˜ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹: {len(self.config['excluded_categories'])}")
        except FileNotFoundError:
            print(f"âš ï¸  Ğ¤Ğ°Ğ¹Ğ» {config_file} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ")
    
    def save_config(self, config_file='scraper_config.json'):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸"""
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ² {config_file}")
    
    def get_all_counties(self):
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ğ¾ĞºÑ€ÑƒĞ³Ğ¾Ğ²"""
        all_counties = []
        for region, counties in self.counties.items():
            for county in counties:
                county['region'] = region
                all_counties.append(county)
        return all_counties
    
    async def scrape(self, headless=True):
        """
        ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        
        Args:
            headless: True - Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¹, False - Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¹
        """
        if not self.config['selected_counties']:
            print("âŒ ĞĞµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ñ‹ Ğ¾ĞºÑ€ÑƒĞ³Ğ° Ğ´Ğ»Ñ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ!")
            print("   Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ set_counties() Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ Ñ„Ğ°Ğ¹Ğ» scraper_config.json")
            return []
        
        print(f"\nğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ BizBuySell.com (Florida)")
        print(f"ğŸ“ ĞĞºÑ€ÑƒĞ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {len(self.config['selected_counties'])}")
        print(f"ğŸ¯ Ğ˜ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹: {len(self.config['excluded_categories'])}")
        print(f"ğŸ­ Ğ ĞµĞ¶Ğ¸Ğ¼ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ°: {'ĞĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¹' if headless else 'Ğ’Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¹'}")
        print("-" * 70)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            page = await context.new_page()
            
            # Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ³Ğ°
            for county_id in self.config['selected_counties']:
                county_info = self.get_county_info(county_id)
                if not county_info:
                    print(f"âš ï¸  ĞĞºÑ€ÑƒĞ³ {county_id} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² Ğ±Ğ°Ğ·Ğµ")
                    continue
                
                print(f"\nğŸ“– Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: {county_info['name']}")
                
                url = f"{self.base_url}/florida/{county_id}-county-businesses-for-sale/"
                
                try:
                    businesses = await self.scrape_county(page, url, county_info)
                    print(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹: {len(businesses)}")
                    self.businesses.extend(businesses)
                    
                except Exception as e:
                    print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ {county_info['name']}: {str(e)}")
                    continue
            
            await browser.close()
        
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼
        if self.config['excluded_categories']:
            original_count = len(self.businesses)
            self.businesses = [
                b for b in self.businesses 
                if b.get('niche') not in self.config['excluded_categories']
            ]
            print(f"\nğŸ” ĞÑ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: {original_count - len(self.businesses)} Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹")
        
        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ñ x Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ°
        for business in self.businesses:
            business['multiplier'] = self.calculate_multiplier(
                business.get('revenue'), 
                business.get('sde')
            )
        
        print(f"\nâœ¨ Ğ¡Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½! Ğ’ÑĞµĞ³Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ¾: {len(self.businesses)} Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹")
        return self.businesses
    
    async def scrape_county(self, page, url, county_info):
        """Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ³Ğ°"""
        businesses = []
        
        for page_num in range(1, self.config['max_pages_per_county'] + 1):
            page_url = url if page_num == 1 else f"{url}?page={page_num}"
            
            try:
                await page.goto(page_url, wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(random.uniform(2, 4))
                
                # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
                page_businesses = await self.extract_listings(page, county_info)
                
                if not page_businesses:
                    print(f"   ğŸ“„ Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {page_num}: Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹, Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ÑÑ")
                    break
                
                print(f"   ğŸ“„ Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {page_num}: {len(page_businesses)} Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹")
                businesses.extend(page_businesses)
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
                has_next = await page.query_selector('a[rel="next"]')
                if not has_next:
                    break
                    
            except Exception as e:
                print(f"   âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ {page_num}: {str(e)}")
                break
        
        return businesses
    
    async def extract_listings(self, page, county_info):
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ÑĞºĞ° Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹"""
        businesses = []
        
        try:
            # Ğ–Ğ´ĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°
            await page.wait_for_selector('[class*="BusinessProfileCard"], [class*="listing"], article', timeout=10000)
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹
            listings = await page.query_selector_all('[class*="BusinessProfileCard"], article[class*="listing"]')
            
            for listing in listings:
                try:
                    business = await self.extract_business_data(listing, county_info)
                    if business and business.get('title'):
                        businesses.append(business)
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"   âš ï¸  ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ: {str(e)}")
        
        return businesses
    
    async def extract_business_data(self, listing, county_info):
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ñ"""
        business = {
            'id': None,
            'title': None,
            'description': None,
            'price': None,
            'sde': None,
            'revenue': None,
            'niche': None,
            'location': f"{county_info['name']}, Florida",
            'county': county_info['name'],
            'county_id': county_info['id'],
            'region': county_info['region'],
            'sourceUrl': None,
            'source': 'BizBuySell',
            'foundDate': datetime.now().strftime('%Y-%m-%d'),
            'lastModified': datetime.now().strftime('%Y-%m-%d')
        }
        
        try:
            # Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
            title_elem = await listing.query_selector('h2, h3, [class*="title"]')
            if title_elem:
                business['title'] = (await title_elem.text_content()).strip()
            
            # ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
            desc_elem = await listing.query_selector('[class*="description"], [class*="summary"], p')
            if desc_elem:
                business['description'] = (await desc_elem.text_content()).strip()[:500]
            
            # Ğ¦ĞµĞ½Ğ° (Asking Price)
            price_elem = await listing.query_selector('[class*="price"], [class*="asking"]')
            if price_elem:
                price_text = await price_elem.text_content()
                business['price'] = self.extract_number(price_text)
            
            # Cash Flow / SDE
            sde_elem = await listing.query_selector('[class*="cash"], [class*="sde"], [class*="cashflow"]')
            if sde_elem:
                sde_text = await sde_elem.text_content()
                business['sde'] = self.extract_number(sde_text)
            
            # Revenue / Gross
            revenue_elem = await listing.query_selector('[class*="revenue"], [class*="gross"], [class*="sales"]')
            if revenue_elem:
                revenue_text = await revenue_elem.text_content()
                business['revenue'] = self.extract_number(revenue_text)
            
            # ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ
            category_elem = await listing.query_selector('[class*="category"], [class*="industry"], [class*="type"]')
            if category_elem:
                business['niche'] = (await category_elem.text_content()).strip()
            
            # Ğ¡ÑÑ‹Ğ»ĞºĞ°
            link_elem = await listing.query_selector('a[href]')
            if link_elem:
                href = await link_elem.get_attribute('href')
                if href:
                    if href.startswith('/'):
                        business['sourceUrl'] = f"{self.base_url}{href}"
                    elif href.startswith('http'):
                        business['sourceUrl'] = href
            
            # ID Ğ¸Ğ· URL
            if business['sourceUrl']:
                business['id'] = business['sourceUrl'].split('/')[-1] or business['sourceUrl'].split('/')[-2]
            else:
                business['id'] = f"biz_{datetime.now().timestamp()}"
                
        except Exception as e:
            print(f"   âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {str(e)}")
        
        return business
    
    def extract_number(self, text):
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
        if not text:
            return None
        
        import re
        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ $, Ğ·Ğ°Ğ¿ÑÑ‚Ñ‹Ğµ
        cleaned = text.replace('$', '').replace(',', '').strip()
        
        # Ğ˜Ñ‰ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ¾ (Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ K, M)
        match = re.search(r'(\d+(?:\.\d+)?)\s*([KM])?', cleaned, re.IGNORECASE)
        if match:
            number = float(match.group(1))
            suffix = match.group(2)
            
            if suffix:
                suffix = suffix.upper()
                if suffix == 'K':
                    number *= 1000
                elif suffix == 'M':
                    number *= 1000000
            
            return int(number)
        
        return None
    
    def calculate_multiplier(self, revenue, sde):
        """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ñ Revenue/SDE"""
        if revenue and sde and sde > 0:
            multiplier = revenue / sde
            return round(multiplier, 1)
        return None
    
    def get_county_info(self, county_id):
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾ĞºÑ€ÑƒĞ³Ğµ Ğ¿Ğ¾ ID"""
        for region, counties in self.counties.items():
            for county in counties:
                if county['id'] == county_id:
                    return {**county, 'region': region}
        return None
    
    def set_counties(self, county_ids):
        """Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¾ĞºÑ€ÑƒĞ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"""
        self.config['selected_counties'] = county_ids
        print(f"âœ… Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ¾ Ğ¾ĞºÑ€ÑƒĞ³Ğ¾Ğ²: {len(county_ids)}")
    
    def set_excluded_categories(self, categories):
        """Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹"""
        self.config['excluded_categories'] = categories
        print(f"âœ… Ğ˜ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹: {len(categories)}")
    
    def save_to_json(self, filename='bizbuysell_florida_data.json'):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.businesses, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² {filename}")
    
    def save_to_csv(self, filename='bizbuysell_florida_data.csv'):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² CSV"""
        if not self.businesses:
            print("âš ï¸  ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ")
            return
        
        keys = ['id', 'title', 'description', 'price', 'sde', 'revenue', 'multiplier', 
                'niche', 'location', 'county', 'region', 'sourceUrl', 'source', 
                'foundDate', 'lastModified']
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            for business in self.businesses:
                row = {k: business.get(k, '') for k in keys}
                writer.writerow(row)
        
        print(f"ğŸ’¾ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² {filename}")
    
    def print_summary(self):
        """Ğ’Ñ‹Ğ²Ğ¾Ğ´ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸"""
        if not self.businesses:
            print("ğŸ“Š ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ")
            return
        
        print("\n" + "="*70)
        print("ğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ Ğ¡ĞĞ‘Ğ ĞĞĞĞ«Ğ¥ Ğ”ĞĞĞĞ«Ğ¥")
        print("="*70)
        
        print(f"\nğŸ“ˆ Ğ’ÑĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹: {len(self.businesses)}")
        
        # ĞŸĞ¾ Ğ¾ĞºÑ€ÑƒĞ³Ğ°Ğ¼
        counties_stat = {}
        for b in self.businesses:
            county = b.get('county', 'Unknown')
            counties_stat[county] = counties_stat.get(county, 0) + 1
        
        print(f"\nğŸ“ ĞŸĞ¾ Ğ¾ĞºÑ€ÑƒĞ³Ğ°Ğ¼:")
        for county, count in sorted(counties_stat.items(), key=lambda x: x[1], reverse=True):
            print(f"   â€¢ {county}: {count} Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹")
        
        # Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸
        prices = [b['price'] for b in self.businesses if b['price']]
        if prices:
            avg_price = sum(prices) / len(prices)
            print(f"\nğŸ’° Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ°: ${avg_price:,.0f}")
        
        sdes = [b['sde'] for b in self.businesses if b['sde']]
        if sdes:
            avg_sde = sum(sdes) / len(sdes)
            print(f"ğŸ“Š Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ SDE: ${avg_sde:,.0f}")
        
        revenues = [b['revenue'] for b in self.businesses if b['revenue']]
        if revenues:
            avg_revenue = sum(revenues) / len(revenues)
            print(f"ğŸ’µ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Revenue: ${avg_revenue:,.0f}")
        
        # ĞœĞ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸
        multipliers = [b['multiplier'] for b in self.businesses if b['multiplier']]
        if multipliers:
            avg_mult = sum(multipliers) / len(multipliers)
            print(f"ğŸ“ˆ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒ: x{avg_mult:.1f}")
        
        # ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
        niches = {}
        for b in self.businesses:
            if b['niche']:
                niches[b['niche']] = niches.get(b['niche'], 0) + 1
        
        if niches:
            print(f"\nğŸ·ï¸  Ğ¢Ğ¾Ğ¿-5 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹:")
            sorted_niches = sorted(niches.items(), key=lambda x: x[1], reverse=True)[:5]
            for niche, count in sorted_niches:
                print(f"   â€¢ {niche}: {count} Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹")
        
        print("\n" + "="*70)


async def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     BizBuySell Florida - ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¡Ğ±Ğ¾Ñ€Ñ‰Ğ¸Ğº Ğ”Ğ°Ğ½Ğ½Ñ‹Ñ…    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    scraper = BizBuySellFloridaScraper()
    
    # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ
    scraper.load_config()
    
    # Ğ•ÑĞ»Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€
    if not scraper.config['selected_counties']:
        print("\nâš™ï¸  ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ:")
        print("   Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ¿-5 Ğ¾ĞºÑ€ÑƒĞ³Ğ¾Ğ² Florida\n")
        
        # Ğ¢Ğ¾Ğ¿-5 Ğ¾ĞºÑ€ÑƒĞ³Ğ¾Ğ² Ğ¿Ğ¾ Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ñ
        scraper.set_counties([
            'miami-dade',
            'broward', 
            'palm-beach',
            'hillsborough',
            'orange'
        ])
        
        # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹
        scraper.set_excluded_categories([
            'Restaurant',
            'Retail'
        ])
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ°
        scraper.save_config()
    
    # ĞĞĞ¡Ğ¢Ğ ĞĞ™ĞšĞ˜
    MAX_PAGES_PER_COUNTY = 3  # Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¾ĞºÑ€ÑƒĞ³
    HEADLESS = False  # True = Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¹, False = Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€
    
    scraper.config['max_pages_per_county'] = MAX_PAGES_PER_COUNTY
    
    try:
        # Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        await scraper.scrape(headless=HEADLESS)
        
        # Ğ’Ñ‹Ğ²Ğ¾Ğ´ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸
        scraper.print_summary()
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        scraper.save_to_json()
        scraper.save_to_csv()
        
        print("\nâœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹:")
        print("   ğŸ“„ bizbuysell_florida_data.json")
        print("   ğŸ“„ bizbuysell_florida_data.csv")
        print("   ğŸ“„ scraper_config.json")
        
    except Exception as e:
        print(f"\nâŒ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
